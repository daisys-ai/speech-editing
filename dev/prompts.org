* Setup

In this folder you will find an HTML/JS-only static page that gives a nice
interface for editing the timings of words in a sentence.

You can drag the edges of words, and if you click on a word, you see the
individual phonemes.

I'd like you to do a few initial improvements, and then make some important
changes. Initial improvements:

1. Can you print out the durations of all phonemes in console.log as I move the
   handles around?  I want to see an array of [phoneme, duration] pairs like
   this: [["k", 3], ["w", 2], ["i", 1], ["k": 3]] (for the word "quick", just an
   example but you should handle the whole sentence).  I want this printing to
   come from a callback on duration changes that I can hook into later for a
   different purpose.
2. Can you add the ability to insert "silence" as a word, if I click between the
   words?  You'll need some sort of subtle icon for this, and a way of tracking
   in the code that there is silence there. Silence blocks should just be
   treated as words. If needed, you can represent them in the text with a
   self-closing "<break/>" tag.
3. Back this thing up with an extremely simple server, perhaps using FastAPI.
   Ideally I want the app to be hostable as a static page but for development it
   can be useful to be serving it, and I may have the need for an endpoint or
   two.

Now for the important changes:

1. Add in the top corner a login button that gives a username and password
   field.  The password field should have masked text, and auto-fill between
   sessions using cookies.  No need to show the fields if the user is already
   logged in, you can just indicate that, and show a logout icon.
2. I want you do add support for the Daisys API.  You can find full instructions
   here: https://daisys-ai.github.io/daisys-api-python/daisys-api-doc.txt
3. What I'd like you to use the API for is: for login, and doing so, save the
   accesss token in a local storage or cookie so that the user remains logged in
   unless he logs out.
4. For text to voice. There is a sentence there already, and when the user
   clicks "Preview timing" I'd like you to use the service to perform TTS using
   /take/generate, get the audio, and play it back. You can have an adequate
   waiting spinner. For a voice, please choose a default voice or create one,
   but it has to be for the "infilling-en" model.  I'd like you to do all of
   this in JavaScript on the client side.  Server is there only for static
   serving of the interface.
5. In fact it should not show the word timings at all until it is able to
   retrieve them from the TakeResponse object. (I will add instructions on how
   to do this later, for now assume all words are composed of arbitrary phonemes
   composed of their letters, and timings are some random small integer.)
   Therefore we should not see the duration editing interface until the first
   preview has occurred.  And we should not see preview button until the user
   has logged in.
6. Each preview should be kept in a history and it should be possible to jump
   back to the audio and durations associated with some item in that history.
   Please make a widget to track historical clicks of the play button and offer
   a delete button for the items.  This can all go into local storage, so you
   should also have "folders" per sentence.
7. By the way that means that after the sentence is established, the user should
   not be able to edit it, so the input dialog should be visually disabled.  You
   should organize the history hierarchically so that I can see previous
   sessions and then drill down into previous states (durations and audio
   examples) from those sessions.

* Refinements

After logging in, the modal disappeared but the login button was still there and
no Preview button appeared, so I couldn't do anything else.

Okay but I need an actual access token for TTS to work. Can you add
documentation on how to switch modes to the README?  Also you should
differentiate between the URL for the daisys API, which I do want to override,
and the URL for authentication.  Please support both DAISYS_API_URL and
DAISYS_AUTH_URL as environment variables.  Use an env file with commented out
lines for each configuration.

Login appears to work, according to console messages, but after login I do not
see the preview button and now it is gone even if I refresh.

Progress, login works and preview almost works. I made some edits so that it
successfully generates a voice. Now it's failing to generate a take, but that's
because the JSON is incorrect. Please refer to the Daisys documentation on what
input to get /takes/generate.

Almost there but you are missing the voice_id field.  Please adjust the code to
determine or generate the voice immediately after login, so that everything is
ready to just create a take on demand.

Almost working, but fetching the audio is incorrect. Please process response
from /takes/generate, poll for 'ready' status, and then request the audio using
the /wav URL, as indicated in the Daisys documentation.

#+begin_quote
One thing to watch for in the /wav endpoint. If it is a redirect, please strip
the Authorization header when loading the new location.
#+end_quote

At this stage it's working fine for previews.  Now to work on regeneration.

* Regeneration

The version of the API you are using supports a new endpoint in the Daisys API
that is not documented.  I will tell you how it works and I want you to use it.

For a given sentence, the first time you generate, you should use the
/takes/generate endpoint just like before.  I would like you to remember the
take_id of this first sentence and we will now re-use it.

The second time you generate, I would like you to use the new endpoint
/takes/regenerate.  POST requests to this endpoint are similar to the /generate
endpoint, but take some new fields:

edit_take_id: Optional[str] = None
text_to_edit: Optional[list[Union[tuple[int, int], str]]] = None
phoneme_durations: Optional[list[list[list[tuple[str, int]]]]] = None # sentence, word, (phoneme, duration)

The edit_take_id is where you should put the take_id from the first generate
command.

The text_to_edit is the sequence of words within the sentence for which
durations have been edited.  You will need to calculate this in the UI.  It's
probably better not to track it but rather to just compare durations with the
original output from the first generation.  Maybe to confirm to the user what
they are doing, you can colour the boxes for words that have been edited in a
slightly different shade.  Each item of the outermost list is a sentence, so if
we're just editing one sentence, it should be a one-item list. For example, if
the original text was,

"Hi there buddy.  How are you?"

and you edited "there buddy", then text_to_edit would be either ["there buddy"]
or [[1, 2]] to indicate words 1 and 2 of the first sentence.  If you edited "How
are" then it would be ["", "How are"] or [[], [0, 1]].

For phoneme_durations, this is where the phonemes and their edited durations
will go.  It is a list of sentences. Each sentence is a list of words. Each word
is a list of tuples which are two items: the phoneme (string) and the duration
(integer) in number of frames.

An example of a response (once the take is 'ready') after asking /takes/generate
to synthesize the sentence "Hello there, I am Daisys!" is:

{"text":"Hello there, I am Daisys!","override_language":null,"style":null,"prosody":{"pitch":2,"pace":-3,"expression":10},"is_blocked":false,"status_webhook":null,"done_webhook":null,"user_data":null,"voice_id":"v01jz4za2jby72gmft9gj3bx5z8","take_id":"t01jz4zackwv2263wmch9dfrzas","status":"ready","timestamp_ms":1751439979132,"info":{"duration":98816,"audio_rate":44100,"normalized_text":["Hello there, I am Daisys!"],"phoneme_durations":[[[["SIL",5],["h",2],["ə",4],["l",8],["oʊ",27]],[["ð",4],["ɛɹ",28],["SIL",30]],[["aɪ",5]],[["æ",3],["m",7]],[["d",7],["eɪ",10],["s",11],["ɪ",12],["s",20],["SIL",10]]]]}}

Now, given this information, you have the durations of each phoneme composing
each word.  I want you to make the following updates to the UI:

1. The word box widths should reflect the total duration of each word, and in
   the phoneme editing view, the boxes should reflect the duration of each phoneme.
2. When phonemes or words are edited, this should be recorded, and the colors of
   the boxes should be slightly changed to reflect that they have been edited.
   I suggest doing this statelessly: do not track edits, but rather just always
   compare the current durations with the known durations from the original
   take. That way if a word is edited such that its phonemes are back to their
   original durations, it will be displayed as unedited, but if any phoneme in a
   word has a different duration, it and its word will be displayed as edited.
3. When the user clicks preview, the new preview added to the history should be
   generated by calling /takes/regeneration.  Provide the original take_id,
   provide the longest span of text that was edited in text_to_edit, and provide
   new durations in phoneme_durations.  Otherwise provide the same text and
   voice as the original take.
4. Add three sliders to the interface called "pitch", "pace", and "expression".
   According to the Daisys documentation these take values between -10 and 10.
   These will control the prosody for the original take.  On subsequent
   regenerations, grey out "pace" since that cannot be controlled, but provide
   the prosody with new values for "pitch" and "expression" if they have been
   changed.

